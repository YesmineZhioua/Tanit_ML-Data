import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Configuration pour de meilleurs graphiques
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 6)

print("="*80)
print("ðŸ“Š PIPELINE COMPLET : EDA + NETTOYAGE + VISUALISATION")
print("="*80 + "\n")


# =============================================================================
# PARTIE 1 : ANALYSE EXPLORATOIRE AVANT NETTOYAGE
# =============================================================================

def load_and_explore_data(csv_path):
    """Charge et explore les donnÃ©es initiales"""
    print("="*80)
    print("1ï¸âƒ£ CHARGEMENT ET EXPLORATION INITIALE")
    print("="*80 + "\n")
    
    df = pd.read_csv(csv_path)
    
    print(f"âœ“ DonnÃ©es chargÃ©es : {csv_path}")
    print(f"  â€¢ Nombre de patients : {len(df)}")
    print(f"  â€¢ Nombre de colonnes : {len(df.columns)}")
    print(f"  â€¢ Taille mÃ©moire : {df.memory_usage(deep=True).sum() / 1024:.2f} KB\n")
    
    print("ðŸ“‹ APERÃ‡U DES DONNÃ‰ES (5 premiÃ¨res lignes) :")
    print("-"*80)
    print(df.head())
    print("-"*80 + "\n")
    
    print("ðŸ” TYPES DE DONNÃ‰ES :")
    print("-"*80)
    print(df.dtypes)
    print("-"*80 + "\n")
    
    print("â„¹ï¸ INFORMATIONS GÃ‰NÃ‰RALES :")
    print("-"*80)
    df.info()
    print("-"*80 + "\n")
    
    print("ðŸ“ˆ STATISTIQUES DESCRIPTIVES :")
    print("-"*80)
    print(df.describe())
    print("-"*80 + "\n")
    
    return df


def analyze_missing_values(df):
    """Analyse dÃ©taillÃ©e des valeurs manquantes"""
    print("="*80)
    print("2ï¸âƒ£ ANALYSE DES VALEURS MANQUANTES")
    print("="*80 + "\n")
    
    missing = df.isnull().sum()
    missing_pct = (missing / len(df)) * 100
    
    missing_df = pd.DataFrame({
        'Colonne': missing.index,
        'Valeurs manquantes': missing.values,
        'Pourcentage (%)': missing_pct.values
    })
    
    missing_df = missing_df[missing_df['Valeurs manquantes'] > 0].sort_values(
        'Valeurs manquantes', ascending=False
    )
    
    if len(missing_df) > 0:
        print("âš ï¸ VALEURS MANQUANTES DÃ‰TECTÃ‰ES :")
        print("-"*80)
        print(missing_df.to_string(index=False))
        print("-"*80 + "\n")
        
        # Visualisation
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        missing_df.plot(x='Colonne', y='Valeurs manquantes', kind='bar', ax=ax1, color='coral')
        ax1.set_title('Nombre de valeurs manquantes par colonne', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Nombre')
        ax1.set_xlabel('Colonnes')
        ax1.tick_params(axis='x', rotation=45)
        
        missing_df.plot(x='Colonne', y='Pourcentage (%)', kind='bar', ax=ax2, color='salmon')
        ax2.set_title('Pourcentage de valeurs manquantes par colonne', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Pourcentage (%)')
        ax2.set_xlabel('Colonnes')
        ax2.tick_params(axis='x', rotation=45)
        ax2.axhline(y=40, color='red', linestyle='--', label='Seuil critique 40%')
        ax2.legend()
        
        plt.tight_layout()
        plt.savefig('eda_01_missing_values_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ðŸ’¾ Graphique sauvegardÃ© : eda_01_missing_values_analysis.png\n")
        
        # Heatmap
        plt.figure(figsize=(12, 6))
        sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='RdYlGn_r')
        plt.title('Heatmap des valeurs manquantes (rouge = manquant)', fontsize=14, fontweight='bold')
        plt.xlabel('Colonnes')
        plt.ylabel('Patients')
        plt.tight_layout()
        plt.savefig('eda_02_missing_values_heatmap.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ðŸ’¾ Graphique sauvegardÃ© : eda_02_missing_values_heatmap.png\n")
    else:
        print("âœ… Aucune valeur manquante dÃ©tectÃ©e !\n")
    
    return missing_df


def analyze_distributions(df):
    """Analyse les distributions des variables numÃ©riques"""
    print("="*80)
    print("3ï¸âƒ£ ANALYSE DES DISTRIBUTIONS")
    print("="*80 + "\n")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if 'Patient_id' in numeric_cols:
        numeric_cols.remove('Patient_id')
    
    print(f"ðŸ“Š Variables numÃ©riques : {', '.join(numeric_cols)}\n")
    
    n_cols = len(numeric_cols)
    n_rows = (n_cols + 2) // 3
    
    fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))
    axes = axes.flatten() if n_cols > 1 else [axes]
    
    for idx, col in enumerate(numeric_cols):
        ax = axes[idx]
        data = df[col].dropna()
        
        if len(data) > 0:
            ax.hist(data, bins=20, color='skyblue', edgecolor='black', alpha=0.7)
            ax.set_title(f'Distribution de {col}', fontweight='bold')
            ax.set_xlabel(col)
            ax.set_ylabel('FrÃ©quence')
            
            mean_val = data.mean()
            median_val = data.median()
            ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, 
                      label=f'Moyenne: {mean_val:.2f}')
            ax.axvline(median_val, color='green', linestyle='--', linewidth=2, 
                      label=f'MÃ©diane: {median_val:.2f}')
            ax.legend()
    
    for idx in range(n_cols, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.savefig('eda_03_distributions_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("ðŸ’¾ Graphique sauvegardÃ© : eda_03_distributions_analysis.png\n")


def analyze_categorical(df):
    """Analyse les variables catÃ©gorielles"""
    print("="*80)
    print("4ï¸âƒ£ ANALYSE DES VARIABLES CATÃ‰GORIELLES")
    print("="*80 + "\n")
    
    categorical_cols = ['Protocol', 'Patient_Response']
    
    for col in categorical_cols:
        if col in df.columns:
            print(f"ðŸ“Š Distribution de {col} :")
            print("-"*80)
            value_counts = df[col].value_counts(dropna=False)
            value_pct = df[col].value_counts(dropna=False, normalize=True) * 100
            
            result = pd.DataFrame({
                'Valeur': value_counts.index,
                'Nombre': value_counts.values,
                'Pourcentage (%)': value_pct.values
            })
            print(result.to_string(index=False))
            print("-"*80 + "\n")
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    if 'Protocol' in df.columns:
        df['Protocol'].value_counts().plot(kind='bar', ax=axes[0], color='lightcoral')
        axes[0].set_title('Distribution des Protocoles', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('Protocol')
        axes[0].set_ylabel('Nombre de patients')
        axes[0].tick_params(axis='x', rotation=45)
    
    if 'Patient_Response' in df.columns:
        df['Patient_Response'].value_counts().plot(kind='bar', ax=axes[1], color='lightgreen')
        axes[1].set_title('Distribution des RÃ©ponses Patients', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('Patient Response')
        axes[1].set_ylabel('Nombre de patients')
        axes[1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig('eda_04_categorical_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("ðŸ’¾ Graphique sauvegardÃ© : eda_04_categorical_analysis.png\n")


def detect_outliers(df):
    """DÃ©tecte les valeurs aberrantes"""
    print("="*80)
    print("5ï¸âƒ£ DÃ‰TECTION DES OUTLIERS")
    print("="*80 + "\n")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if 'Patient_id' in numeric_cols:
        numeric_cols.remove('Patient_id')
    if 'Cycle_number' in numeric_cols:
        numeric_cols.remove('Cycle_number')
    
    n_cols = len(numeric_cols)
    n_rows = (n_cols + 2) // 3
    
    fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))
    axes = axes.flatten() if n_cols > 1 else [axes]
    
    print("ðŸ“Š Outliers dÃ©tectÃ©s par variable :")
    print("-"*80)
    
    for idx, col in enumerate(numeric_cols):
        ax = axes[idx]
        data = df[col].dropna()
        
        if len(data) > 0:
            ax.boxplot(data, vert=True)
            ax.set_title(f'Boxplot de {col}', fontweight='bold')
            ax.set_ylabel(col)
            
            Q1 = data.quantile(0.25)
            Q3 = data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = data[(data < lower_bound) | (data > upper_bound)]
            
            print(f"  â€¢ {col:20s} : {len(outliers)} outliers")
            
            if len(outliers) > 0:
                ax.text(0.5, 0.95, f'{len(outliers)} outliers', 
                       transform=ax.transAxes, ha='center', va='top',
                       bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))
    
    print("-"*80 + "\n")
    
    for idx in range(n_cols, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.savefig('eda_05_outliers_detection.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("ðŸ’¾ Graphique sauvegardÃ© : eda_05_outliers_detection.png\n")


def analyze_correlations(df):
    """Analyse les corrÃ©lations entre variables"""
    print("="*80)
    print("6ï¸âƒ£ ANALYSE DES CORRÃ‰LATIONS")
    print("="*80 + "\n")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if 'Patient_id' in numeric_cols:
        numeric_cols.remove('Patient_id')
    
    corr_matrix = df[numeric_cols].corr()
    
    print("ðŸ“Š MATRICE DE CORRÃ‰LATION :")
    print("-"*80)
    print(corr_matrix.round(3))
    print("-"*80 + "\n")
    
    print("ðŸ” CORRÃ‰LATIONS FORTES (|r| > 0.5) :")
    print("-"*80)
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_val = corr_matrix.iloc[i, j]
            if abs(corr_val) > 0.5:
                var1 = corr_matrix.columns[i]
                var2 = corr_matrix.columns[j]
                direction = "positive" if corr_val > 0 else "nÃ©gative"
                emoji = "ðŸ“ˆ" if corr_val > 0 else "ðŸ“‰"
                print(f"  {emoji} {var1} â†” {var2} : r = {corr_val:.3f} ({direction})")
    print("-"*80 + "\n")
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                square=True, linewidths=1, cbar_kws={"shrink": 0.8},
                fmt='.2f', vmin=-1, vmax=1)
    plt.title('Matrice de CorrÃ©lation', fontsize=16, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.savefig('eda_06_correlation_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("ðŸ’¾ Graphique sauvegardÃ© : eda_06_correlation_matrix.png\n")


def run_complete_eda(csv_path):
    """ExÃ©cute l'analyse exploratoire complÃ¨te"""
    print("\n" + "ðŸ” ANALYSE EXPLORATOIRE COMPLÃˆTE (EDA)")
    print("="*80 + "\n")
    
    # 1. Charger et explorer
    df = load_and_explore_data(csv_path)
    
    # 2. Valeurs manquantes
    missing_df = analyze_missing_values(df)
    
    # 3. Distributions
    analyze_distributions(df)
    
    # 4. Variables catÃ©gorielles
    analyze_categorical(df)
    
    # 5. Outliers
    detect_outliers(df)
    
    # 6. CorrÃ©lations
    analyze_correlations(df)
    
    print("="*80)
    print("âœ… ANALYSE EXPLORATOIRE TERMINÃ‰E")
    print("="*80)
    print("\nðŸ“Š Graphiques gÃ©nÃ©rÃ©s :")
    print("  â€¢ eda_01_missing_values_analysis.png")
    print("  â€¢ eda_02_missing_values_heatmap.png")
    print("  â€¢ eda_03_distributions_analysis.png")
    print("  â€¢ eda_04_categorical_analysis.png")
    print("  â€¢ eda_05_outliers_detection.png")
    print("  â€¢ eda_06_correlation_matrix.png\n")
    
    return df


# =============================================================================
# PARTIE 2 : NETTOYAGE DES DONNÃ‰ES (Ã  importer de votre module)
# =============================================================================

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer


class IVFMedicalDataCleaner:
    """Nettoyage avec AFC prÃ©servÃ©e"""
    
    def __init__(self, csv_path):
        self.csv_path = csv_path
        self.df_original = None
        self.df_cleaned = None
        self.cleaning_report = {}
        self.iterative_imputer = IterativeImputer(max_iter=10, random_state=42)
    
    def load_data(self):
        self.df_original = pd.read_csv(self.csv_path)
        self.df_cleaned = self.df_original.copy()
        return self.df_original
    
    def clean_pipeline(self):
        """Pipeline de nettoyage simplifiÃ© pour dÃ©monstration"""
        print("\n" + "ðŸ§¹ NETTOYAGE DES DONNÃ‰ES")
        print("="*80 + "\n")
        
        df = self.df_cleaned
        
        # 1. Imputation Age
        if 'Age' in df.columns and df['Age'].isnull().sum() > 0:
            median_age = df['Age'].median()
            df['Age'].fillna(median_age, inplace=True)
            print(f"âœ“ Age imputÃ© : mÃ©diane = {median_age:.1f}")
        
        # 2. Extraction numÃ©riques
        if 'AMH' in df.columns and df['AMH'].dtype == 'object':
            df['AMH_numeric'] = df['AMH'].astype(str).str.extract(r'(\d+\.?\d*)')[0].astype(float)
            print(f"âœ“ AMH_numeric extrait")
        
        if 'E2_day5' in df.columns and df['E2_day5'].dtype == 'object':
            df['E2_day5_numeric'] = df['E2_day5'].astype(str).str.extract(r'(\d+\.?\d*)')[0].astype(float)
            print(f"âœ“ E2_day5_numeric extrait")
        
        # 3. Imputation simples (mÃ©diane)
        for var in ['AMH_numeric', 'n_Follicles', 'E2_day5_numeric']:
            if var in df.columns and df[var].isnull().sum() > 0:
                median_val = df[var].median()
                df[var].fillna(median_val, inplace=True)
                print(f"âœ“ {var} imputÃ© : mÃ©diane = {median_val:.2f}")
        
        # 4. Imputation AFC intelligente
        if 'AFC' in df.columns and df['AFC'].isnull().sum() > 0:
            n_missing = df['AFC'].isnull().sum()
            
            imputation_features = []
            if 'AMH_numeric' in df.columns:
                imputation_features.append('AMH_numeric')
            if 'Age' in df.columns:
                imputation_features.append('Age')
            if 'n_Follicles' in df.columns:
                imputation_features.append('n_Follicles')
            imputation_features.append('AFC')
            
            imputed_values = self.iterative_imputer.fit_transform(df[imputation_features])
            afc_idx = imputation_features.index('AFC')
            df['AFC'] = imputed_values[:, afc_idx]
            
            print(f"âœ“ AFC imputÃ© intelligemment : {n_missing} valeurs (r=0.77 avec AMH)")
        
        # 5. Normalisation RobustScaler
        for var in ['Age', 'AMH_numeric', 'n_Follicles', 'E2_day5_numeric', 'AFC']:
            if var in df.columns:
                median_val = df[var].median()
                Q1 = df[var].quantile(0.25)
                Q3 = df[var].quantile(0.75)
                IQR = Q3 - Q1
                if IQR != 0:
                    df[f'{var}_robust'] = (df[var] - median_val) / IQR
        
        print(f"âœ“ Normalisation RobustScaler appliquÃ©e")
        
        if 'Protocol' in df.columns:
    # Nettoyage : minuscules + suppression espaces superflus
                df['Protocol_clean'] = df['Protocol'].astype(str).str.strip().str.lower()

                # Extraire tous les types uniques
                unique_protocols = df['Protocol_clean'].unique()
                print("ðŸ”¹ Protocoles uniques trouvÃ©s :", unique_protocols)

                # CrÃ©er des listes pour les 3 catÃ©gories
                agonist_list = [p for p in unique_protocols if 'agonist' in p and 'flex' not in p and 'fix' not in p]
                flexible_antagonist_list = [p for p in unique_protocols if 'flex' in p]
                fixed_antagonist_list = [p for p in unique_protocols if 'fix' in p]

                print("Agonist :", agonist_list)
                print("Flexible Antagonist :", flexible_antagonist_list)
                print("Fixed Antagonist :", fixed_antagonist_list)

                # CrÃ©ation d'un dictionnaire de mapping automatique
                protocol_mapping = {}
                for p in agonist_list:
                    protocol_mapping[p] = 0
                for p in flexible_antagonist_list:
                    protocol_mapping[p] = 1
                for p in fixed_antagonist_list:
                    protocol_mapping[p] = 2

                # Encodage
                df['Protocol_encoded'] = df['Protocol_clean'].map(protocol_mapping)

                # Pour les valeurs inconnues, assigner -1
                df['Protocol_encoded'].fillna(-1, inplace=True)

                print(f"âœ“ Protocol encodÃ© : valeurs uniques aprÃ¨s encodage -> {df['Protocol_encoded'].unique()}")

            
        # 7. Doublons
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            df = df.drop_duplicates(keep='first')
            print(f"âœ“ {duplicates} doublons supprimÃ©s")
        
        self.df_cleaned = df
        self.cleaning_report['rows_after'] = len(df)
        
        print("\nâœ… Nettoyage terminÃ©")
        print(f"  â€¢ Patients : {len(df)}")
        print(f"  â€¢ Colonnes : {len(df.columns)}\n")
        
        return df
    
    def save_cleaned_data(self, output_path):
        self.df_cleaned.to_csv(output_path, index=False)
        print(f"ðŸ’¾ DonnÃ©es nettoyÃ©es sauvegardÃ©es : {output_path}\n")


# =============================================================================
# PARTIE 3 : VISUALISATION AVANT/APRÃˆS
# =============================================================================

class CleaningVisualizer:
    """Visualisation avant/aprÃ¨s nettoyage"""
    
    def __init__(self, df_before, df_after, output_dir='cleaning_viz'):
        self.df_before = df_before
        self.df_after = df_after
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def viz_missing_comparison(self):
        """Comparaison valeurs manquantes"""
        print("ðŸ“Š GÃ©nÃ©ration visualisation : Valeurs manquantes avant/aprÃ¨s")
        
        missing_before = self.df_before.isnull().sum()
        missing_before = missing_before[missing_before > 0]
        
        common_cols = [col for col in missing_before.index if col in self.df_after.columns]
        missing_after = self.df_after[common_cols].isnull().sum()
        
        comparison = pd.DataFrame({
            'Avant': missing_before[common_cols],
            'AprÃ¨s': missing_after
        })
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
        x = np.arange(len(common_cols))
        width = 0.35
        
        ax1.bar(x - width/2, comparison['Avant'], width, label='Avant', color='coral', alpha=0.8)
        ax1.bar(x + width/2, comparison['AprÃ¨s'], width, label='AprÃ¨s', color='skyblue', alpha=0.8)
        ax1.set_xlabel('Variables')
        ax1.set_ylabel('Valeurs manquantes')
        ax1.set_title('Comparaison valeurs manquantes')
        ax1.set_xticks(x)
        ax1.set_xticklabels(common_cols, rotation=45, ha='right')
        ax1.legend()
        
        reduction = ((comparison['Avant'] - comparison['AprÃ¨s']) / comparison['Avant'] * 100)
        colors = ['#2ecc71' if val > 0 else '#e74c3c' for val in reduction]
        
        ax2.barh(common_cols, reduction, color=colors, alpha=0.8)
        ax2.set_xlabel('RÃ©duction (%)')
        ax2.set_title('AmÃ©lioration par variable')
        ax2.axvline(0, color='black', linewidth=0.8)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'viz_01_missing_comparison.png', dpi=300)
        plt.show()
        print(f"  âœ“ viz_01_missing_comparison.png\n")
    
    def viz_afc_quality(self):
        """QualitÃ© imputation AFC"""
        if 'AFC' not in self.df_before.columns or 'AFC' not in self.df_after.columns:
            return
        
        print("ðŸ“Š GÃ©nÃ©ration visualisation : QualitÃ© imputation AFC")
        
        afc_before = self.df_before['AFC'].copy()
        afc_after = self.df_after['AFC'].copy()
        was_missing = afc_before.isnull()
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # Distribution
        afc_original = afc_before.dropna()
        afc_complete = afc_after.dropna()
        
        axes[0].hist(afc_original, bins=20, alpha=0.6, label='Original', 
                    color='coral', edgecolor='black')
        axes[0].hist(afc_complete, bins=20, alpha=0.6, label='AprÃ¨s imputation', 
                    color='skyblue', edgecolor='black')
        axes[0].set_xlabel('AFC')
        axes[0].set_ylabel('FrÃ©quence')
        axes[0].set_title('Distribution AFC : Avant vs AprÃ¨s')
        axes[0].legend()
        
        # AFC vs AMH
        if 'AMH_numeric' in self.df_after.columns:
            mask_original = ~was_missing
            axes[1].scatter(self.df_after.loc[mask_original, 'AMH_numeric'], 
                          self.df_after.loc[mask_original, 'AFC'],
                          alpha=0.5, s=50, label='Valeurs originales', color='coral')
            
            mask_imputed = was_missing & afc_after.notna()
            axes[1].scatter(self.df_after.loc[mask_imputed, 'AMH_numeric'], 
                          self.df_after.loc[mask_imputed, 'AFC'],
                          alpha=0.7, s=80, marker='s', label='Valeurs imputÃ©es', 
                          color='skyblue', edgecolor='black')
            
            axes[1].set_xlabel('AMH')
            axes[1].set_ylabel('AFC')
            axes[1].set_title('AFC imputÃ© vs AMH (r=0.77)')
            axes[1].legend()
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'viz_02_afc_quality.png', dpi=300)
        plt.show()
        print(f"  âœ“ viz_02_afc_quality.png\n")
    
    def generate_all(self):
        print("\nðŸŽ¨ GÃ©nÃ©ration des visualisations avant/aprÃ¨s\n")
        self.viz_missing_comparison()
        self.viz_afc_quality()
        print("âœ… Visualisations terminÃ©es\n")


# =============================================================================
# PIPELINE COMPLET
# =============================================================================

def run_complete_pipeline(input_csv, output_csv):
    """
    Pipeline complet : EDA â†’ Nettoyage â†’ Visualisation
    """
    
    print("\n" + "="*80)
    print("ðŸš€ PIPELINE COMPLET : EDA + NETTOYAGE + VISUALISATION")
    print("="*80 + "\n")
    
    # Ã‰TAPE 1 : EDA avant nettoyage
    print("Ã‰TAPE 1 : ANALYSE EXPLORATOIRE (AVANT NETTOYAGE)")
    print("="*80)
    df_original = run_complete_eda(input_csv)
    
    # Ã‰TAPE 2 : Nettoyage
    print("\n" + "="*80)
    print("Ã‰TAPE 2 : NETTOYAGE DES DONNÃ‰ES")
    print("="*80)
    cleaner = IVFMedicalDataCleaner(input_csv)
    cleaner.load_data()
    df_cleaned = cleaner.clean_pipeline()
    cleaner.save_cleaned_data(output_csv)
    
    # Ã‰TAPE 3 : Visualisation avant/aprÃ¨s
    print("="*80)
    print("Ã‰TAPE 3 : VISUALISATION AVANT/APRÃˆS")
    print("="*80)
    visualizer = CleaningVisualizer(df_original, df_cleaned)
    visualizer.generate_all()
    
    print("="*80)
    print("âœ… PIPELINE COMPLET TERMINÃ‰")
    print("="*80)
    print("\nðŸ“Š Fichiers gÃ©nÃ©rÃ©s :")
    print("  EDA (6 graphiques) :")
    print("    â€¢ eda_01_missing_values_analysis.png")
    print("    â€¢ eda_02_missing_values_heatmap.png")
    print("    â€¢ eda_03_distributions_analysis.png")
    print("    â€¢ eda_04_categorical_analysis.png")
    print("    â€¢ eda_05_outliers_detection.png")
    print("    â€¢ eda_06_correlation_matrix.png")
    print("\n  Nettoyage :")
    print(f"    â€¢ {output_csv}")
    print("\n  Visualisation avant/aprÃ¨s (2 graphiques) :")
    print("    â€¢ viz_01_missing_comparison.png")
    print("    â€¢ viz_02_afc_quality.png")
    print("\n" + "="*80 + "\n")


# =============================================================================
# UTILISATION
# =============================================================================

if __name__ == "__main__":
    
    # Chemins des fichiers
    input_csv = "C:\\Users\\yesmine\\Desktop\\Tanit\\data\\raw\\patients.csv"
    output_csv = "C:\\Users\\yesmine\\Desktop\\Tanit\\data\\processed\\patients_medical_clean.csv"
    
    # ExÃ©cuter le pipeline complet
    run_complete_pipeline(input_csv, output_csv)